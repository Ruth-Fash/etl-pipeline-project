A. Data Exploration & Basic Analytical Challenges
Top contributors: Which entities (users/customers/products/etc.) account for the top 20% of total volume (e.g., revenue, events, actions)? What percentage of total do they represent?
Time-based activity distribution: How does activity vary by hour of day and day of week? Identify peak and trough periods.
New vs returning: For each week, what fraction of active users are new that week versus returning?
Event sequencing gaps: Identify users who had event A (e.g., “viewed item”) but did not have event B (e.g., “added to cart”) within X hours afterwards.
Rolling aggregates: Compute a 7-day moving average of daily event counts, then flag days where actual count is >1.5× or <0.5× that average.

B. Cohorts, Retention, and Funnel Analysis
Cohort retention: Group users by their signup week. For each cohort, calculate retention rate at 1, 2, and 4 weeks after signup.
Conversion funnel: Define a funnel (e.g., visit → sign-up → purchase). For a given time window, calculate conversion rates at each step and identify the biggest drop-off.
Time to conversion: For users who converted (made a purchase), compute the distribution of time between first session and conversion.
Churn risk proxy: Find users whose activity dropped by >50% compared to their average over the prior period (e.g., compare last 7 days to the previous 30).
Reactivation: Among users inactive for ≥30 days, how many returned and performed any action in the subsequent 14 days? What triggered their return (if you have markers)?

C. Comparative & Relative Metrics
Above/below average performers: Identify entities (users/products/regions) that performed >1 standard deviation above or below the mean on some metric (e.g., spend, frequency).
Top N + “Other” grouping: For a dimension like country or category, list top 5 by revenue and bucket all others into “Other,” showing relative contribution.
Percentage of total over time: For a particular category, show its share of total activity/revenue by month and detect rising/falling trends.
Ranked “last active”: For each user, rank their sessions or purchases and find the second-to-last activity date (helps with detecting declining engagement).
“Sticky” users: Define a metric combining frequency and recency (e.g., weighted score) and rank users—then compare top 5% vs bottom 5% behaviors.

D. Data Quality & Anomaly Detection
Missing data patterns: Detect rows with nulls in critical fields and characterize whether they cluster by source, time, or user.
Duplicates: Find potential duplicate records (same user/action/timestamp within a small epsilon) and quantify their impact on aggregate metrics.
Outlier detection: For numeric fields (amounts, durations), find values outside the 99th percentile or that deviate drastically from rolling norms.
Inconsistent state transitions: If you have status flows (e.g., “pending” → “confirmed” → “shipped”), identify records that jump or regress (e.g., go from “shipped” back to “pending”).
Unexpected gaps: For a time series (per-user heartbeat or daily activity), find users with large unexpected gaps in expected regular activity.

E. Advanced & Composite Challenges
Engagement score synthesis: Define a composite score using multiple normalized behaviors (e.g., session count, average duration, purchase volume) and segment users into quintiles; describe differences.
Sessionization logic: Given raw timestamped events, group them into sessions with a timeout threshold (e.g., 30 minutes of inactivity), then analyze session length distribution.
Lag/lead analysis: For each user, compute time between successive key events and identify users with atypically long or short gaps.
Peer grouping: Cluster users into “peers” by similar behavior (e.g., via quantile bucketing on two or three metrics) and compare conversion or retention across groups.
Recursive relationships: If your data has hierarchical ties (e.g., referrals, parent/child), trace depth or compute cumulative metrics up the chain.

F. Performance / Engineering-Oriented Challenges
Index impact simulation: Pick a slow filter/join query, explain why it’s slow (based on what you’d expect from cardinality), then hypothesize which indexes would help and how they’d change execution.
Materialized view vs live computation: Take a costly aggregation, build it as a materialized view, and design a refresh strategy that balances freshness vs load.
Approximate vs exact counts: For very large distinct counts (e.g., unique users), design an approach to approximate (e.g., hyperloglog-esque via sample) and quantify deviation from exact on your small dataset.
Parameter sensitivity: Create a parameterized query (e.g., time window size), sweep that parameter, and analyze how the output metric (e.g., conversion rate) changes—identify stability or volatility.
Change data capture simulation: Given “before” and “after” snapshots, write logic to classify rows as inserts, updates, or deletes, and generate a delta summary.
